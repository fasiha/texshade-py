<!DOCTYPE html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Texshade: texture-shaded elevation via the fractional-Laplacian operator</title>
  <meta name="description" content="Public-domain Python library for spicing up elevation maps with Math.">
  <meta name="twitter:card" value="summary">
  <meta property="og:title" content="Texshade: texture-shaded elevation via the fractional-Laplacian operator">
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://fasiha.github.io/texshade-py/">
  <meta property="og:image" content="https://fasiha.github.io/texshade-py/hankel-texshade-alpha-0.8-n-500-mmap.png.small.png">
  <meta property="og:description" content="Public-domain Python library for spicing up elevation maps with Math.">

  <style>
    pre,
    code {
      font-family: Menlo, Monaco, "Courier New", monospace;
      font-weight: bold;
    }

    img {
      max-width: 100%;
    }

    blockquote {
      border-left: 3px solid #27ae60;
      padding-left: 1rem;
    }

    /* Adapted from perfectmotherfuckingwebsite */
    div.bookmark {
      padding-top: 0.4em;
      border-bottom: #ddd dotted;
    }

    div.comment:not(:last-child) {
      border-bottom: 2px #ddd dotted;
    }

    pre.unrendered {
      white-space: pre-wrap;
      display: inline;
      font-family: inherit;
    }

    a.emojilink {
      text-decoration: none;
    }

    textarea {
      width: 100%;
      height: 40vh;
      font: 20px/1.3 -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";
    }

    body {
      overflow-wrap: break-word;
      max-width: 80rem;
      margin: 10px auto;
      padding: 0 10px;
      font: 20px/1.3 "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
      /* font-family:  */
      color: #444
    }

    @media (prefers-color-scheme: dark) {
      body {
        color: #ccc;
        background: #111;
      }

      a:link {
        color: #5bf
      }

      a:visited {
        color: #ccf
      }

      div.bookmark {
        border-bottom: #222 dotted;
      }

      div.comment:not(:last-child) {
        border-bottom: 2px #222 dotted;
      }

    }
  </style>

  <style>
    code.sourceCode>span {
      display: inline-block;
      line-height: 1.25;
    }

    code.sourceCode>span {
      color: inherit;
      text-decoration: inherit;
    }

    code.sourceCode>span:empty {
      height: 1.2em;
    }

    .sourceCode {
      overflow: visible;
    }

    code.sourceCode {
      white-space: pre;
      position: relative;
    }

    div.sourceCode {
      margin: 1em 0;
    }

    pre.sourceCode {
      margin: 0;
    }

    @media screen {
      div.sourceCode {
        overflow: auto;
      }
    }

    @media print {
      code.sourceCode {
        white-space: pre-wrap;
      }

      code.sourceCode>span {
        text-indent: -5em;
        padding-left: 5em;
      }
    }

    pre.numberSource code {
      counter-reset: source-line 0;
    }

    pre.numberSource code>span {
      position: relative;
      left: -4em;
      counter-increment: source-line;
    }

    pre.numberSource code>span>a:first-child::before {
      content: counter(source-line);
      position: relative;
      left: -1em;
      text-align: right;
      vertical-align: baseline;
      border: none;
      display: inline-block;
      -webkit-touch-callout: none;
      -webkit-user-select: none;
      -khtml-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
      padding: 0 4px;
      width: 4em;
      background-color: #232629;
      color: #7a7c7d;
    }

    pre.numberSource {
      margin-left: 3em;
      border-left: 1px solid #7a7c7d;
      padding-left: 4px;
    }

    div.sourceCode {
      color: #cfcfc2;
      background-color: #232629;
    }

    @media screen {
      code.sourceCode>span>a:first-child::before {
        text-decoration: underline;
      }
    }

    code span {
      color: #cfcfc2;
    }

    /* Normal */
    code span.al {
      color: #95da4c;
    }

    /* Alert */
    code span.an {
      color: #3f8058;
    }

    /* Annotation */
    code span.at {
      color: #2980b9;
    }

    /* Attribute */
    code span.bn {
      color: #f67400;
    }

    /* BaseN */
    code span.bu {
      color: #7f8c8d;
    }

    /* BuiltIn */
    code span.cf {
      color: #fdbc4b;
    }

    /* ControlFlow */
    code span.ch {
      color: #3daee9;
    }

    /* Char */
    code span.cn {
      color: #27aeae;
    }

    /* Constant */
    code span.co {
      color: #7a7c7d;
    }

    /* Comment */
    code span.cv {
      color: #7f8c8d;
    }

    /* CommentVar */
    code span.do {
      color: #a43340;
    }

    /* Documentation */
    code span.dt {
      color: #2980b9;
    }

    /* DataType */
    code span.dv {
      color: #f67400;
    }

    /* DecVal */
    code span.er {
      color: #da4453;
    }

    /* Error */
    code span.ex {
      color: #0099ff;
    }

    /* Extension */
    code span.fl {
      color: #f67400;
    }

    /* Float */
    code span.fu {
      color: #8e44ad;
    }

    /* Function */
    code span.im {
      color: #27ae60;
    }

    /* Import */
    code span.in {
      color: #c45b00;
    }

    /* Information */
    code span.kw {
      color: #cfcfc2;
    }

    /* Keyword */
    code span.op {
      color: #cfcfc2;
    }

    /* Operator */
    code span.ot {
      color: #27ae60;
    }

    /* Other */
    code span.pp {
      color: #27ae60;
    }

    /* Preprocessor */
    code span.re {
      color: #2980b9;
    }

    /* RegionMarker */
    code span.sc {
      color: #3daee9;
    }

    /* SpecialChar */
    code span.ss {
      color: #da4453;
    }

    /* SpecialString */
    code span.st {
      color: #f44f4f;
    }

    /* String */
    code span.va {
      color: #27aeae;
    }

    /* Variable */
    code span.vs {
      color: #da4453;
    }

    /* VerbatimString */
    code span.wa {
      color: #da4453;
    }

    /* Warning */
  </style>

  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->

</head><h1 id="texshade">Texshade</h1>
<p>Texture-shaded elevation via the fractional-Laplacian operator.</p>
<p><img src="example-twin-peaks.png" alt="Mount Sutro and Twin Peaks, San Francisco, at 1:20,000: texshaded at Œ±=0.8" /> (Above: Mount Sutro and Twin Peaks, San Francisco, at 1:20,000: texshaded at Œ±=0.8)</p>
<p><img src="example-golden-gate.png" alt="Golden Gate Bridge, from the Bay, at 1:20,000: texshaded at Œ±=0.8" /> (Above: Golden Gate Bridge, from the Bay, at 1:20,000: texshaded at Œ±=0.8)</p>
<p><img src="example-marin-headlands.png" alt="Marin County Handlands, at 1:20,000: texshaded at Œ±=0.8" /> (Above: Marin County Handlands, at 1:20,000: texshaded at Œ±=0.8)</p>
<p>Links:</p>
<ul>
<li>if you want to just read this document, go to the <a href="https://fasiha.github.io/texshade-py/">homepage</a>;</li>
<li>if you want to clone the code or open an issue, go to <a href="https://github.com/fasiha/texshade-py/">GitHub</a>;</li>
<li>you can also inspect the module on <a href="https://pypi.org/project/texshade/">PyPI</a>;</li>
<li>you can also get in touch with me, <a href="https://fasiha.github.io/#contact">Ahmed Fasih</a>.</li>
</ul>
<p>Table of contents:</p>
<ul>
<li><a href="#texshade">Texshade</a>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#api">API</a>
<ul>
<li><a href="#texshadefft"><code>texshadeFFT</code></a></li>
<li><a href="#texshadespatial"><code>texshadeSpatial</code></a></li>
<li><a href="#makefilter"><code>makeFilter</code></a></li>
</ul></li>
<li><a href="#tutorial">Tutorial</a>
<ul>
<li><a href="#get-the-tutorial-code">Get the tutorial code</a></li>
<li><a href="#install-dependencies">Install dependencies</a>
<ul>
<li><a href="#install-miniconda">Install Miniconda</a></li>
<li><a href="#create-a-new-conda-env-and-install-dependencies">Create a new conda env and install dependencies</a></li>
</ul></li>
<li><a href="#download-data">Download data</a></li>
<li><a href="#convert-the-elevation-data-to-a-numpy-array">Convert the elevation data to a Numpy array</a></li>
<li><a href="#run-texshade">Run texshade!</a></li>
<li><a href="#clamp-quantize-and-export">Clamp, quantize, and export</a></li>
<li><a href="#rescale-for-the-web">Rescale for the web</a>
<ul>
<li><a href="#original-dem">Original DEM</a></li>
<li><a href="#tex-shaded-dem">Tex-shaded DEM</a></li>
</ul></li>
<li><a href="#spatial-filtering-and-fast-convolution-for-low-memory-usage">Spatial filtering and fast-convolution for low-memory usage</a>
<ul>
<li><a href="#crop-1">Crop 1</a></li>
<li><a href="#crop-2">Crop 2</a></li>
<li><a href="#crop-3">Crop 3</a></li>
</ul></li>
</ul></li>
<li><a href="#developing-in-this-repository">Developing in this repository</a></li>
</ul></li>
</ul>
<h2 id="introduction">Introduction</h2>
<p>The best way to understand texture-shaded elevation is to look at some examples: we gave some at the top of this document, but also take your time at the following links to get a visual sense of world with texture shading.</p>
<ol>
<li>Leland Brown's <a href="http://www.textureshading.com/Home.html">textureshading.com</a> is a great start because that's the inventor of the technique, and the website contrasts raw terrain, texture-shaded terrain, and more conventional hillshaded terrain. (There are also links to papers/slides from 2010 and 2014 describing the technique of texture shading in detail.)</li>
<li>My blog post also contains some pretty images, mashing up various basemaps with texture-shaded terrain: <a href="https://fasiha.github.io/post/texshade/">Texture-shaded Globe</a>.</li>
<li>Zoran ƒåuƒçkoviƒá's <a href="https://landscapearchaeology.org/2021/texture-shading/">QGIS plugin</a> also has some captivating images.</li>
</ol>
<p>In words though, <em>texture shading</em> is a visualization technique for digital elevation maps (DEMs) that highlights the network nature of topography, throwing ridges, canyons, and valleys into sharp relief.</p>
<p>(In <em>more</em> words: it works by applying a specific sharpening filter, called a fractional-Laplacian operator, to the elevation. The filter is applied in the frequency domain.)</p>
<p>This repository contains an open-source public-domain Python/Numpy software library to apply the texture shading algorithm on <em>extremely</em> large datasets that are far too large to fit in your computer's memory.</p>
<p>This is useful because a straightforward implementation of the texture-shading technique requires loading the entire elevation map into memory. For large datasets‚Äîlike the ASTER Global DEM, which comes in at roughly 250 GB compressed‚Äîyou either have to find a computer with a lot of memory, or you have to modify the technique slightly.</p>
<p>So in this repository, we apply a well-known trick from signal processing theory, called the <em>overlap-save method</em>, to avoid loading the entire terrain into memory üòÅ. However, this trick requires us to approximate the exact texture-shading "filter" slightly üòï. In exchange for being able to process huge elevation datasets, you need to accept approximated texture-shaded images‚Äî<em>visually</em> you can barely tell the difference üéÜ!</p>
<blockquote>
<p>(Don't worry, you can use this repo to texture-shade smaller elevation maps as well üòä!)</p>
</blockquote>
<h2 id="installation">Installation</h2>
<p>To install this library:</p>
<pre><code>$ python -m pip install texshade</code></pre>
<p>To use it, in your Python code:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">import</span> texshade</span></code></pre></div>
<h2 id="api">API</h2>
<p>There are just three functions in the API. This can serve as a quick introduction to the algorithm itself.</p>
<h3 id="texshadefft"><code>texshadeFFT</code></h3>
<p>API:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="kw">def</span> texshadeFFT(x: np.ndarray, alpha: <span class="bu">float</span>) <span class="op">-&gt;</span> np.ndarray</span></code></pre></div>
<p>This applies the texture shading algorithm to an array <code>x</code> of elevation data, with a shading detail factor <code>alpha</code> &gt;= 0.</p>
<p><em>This is the exact, high-memory, classic implementation of texture shading.</em> It computes a real-only FFT of the entire array <code>x</code>. Use this if your <code>x</code> fits comfortably in RAM: this function is fast and exact.</p>
<p><code>alpha</code> is the shading detail factor, i.e., the power of the fractional-Laplacian operator. <code>alpha=0</code> means no detail (output is the same as the input). <code>alpha=2.0</code> is the full (non-fractional) Laplacian operator and is probably too high. <code>alpha &lt;= 1.0</code> seem aesthetically pleasing.</p>
<h3 id="texshadespatial"><code>texshadeSpatial</code></h3>
<p>API:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="kw">def</span> texshadeSpatial(</span>
<span id="cb4-2"><a href="#cb4-2"></a>    x: np.ndarray,</span>
<span id="cb4-3"><a href="#cb4-3"></a>    alpha: Optional[<span class="bu">float</span>] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb4-4"><a href="#cb4-4"></a>    nDiameter: Optional[<span class="bu">int</span>] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb4-5"><a href="#cb4-5"></a>    <span class="bu">filter</span>: Optional[np.ndarray] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb4-6"><a href="#cb4-6"></a>    <span class="co"># ols kwargs</span></span>
<span id="cb4-7"><a href="#cb4-7"></a>    size<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb4-8"><a href="#cb4-8"></a>    nfft<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb4-9"><a href="#cb4-9"></a>    out<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb4-10"><a href="#cb4-10"></a>    <span class="op">**</span>kwargs</span>
<span id="cb4-11"><a href="#cb4-11"></a>) <span class="op">-&gt;</span> np.ndarray</span></code></pre></div>
<p><strong>This is the low-memory approximation of the texture shading algorithm</strong>. It convolves the input array with an FIR (finite impulse response) approximation of the true fractional-Laplacian operator using an FFT-accelerated overlap-save algorithm ("fast-convolution"). This allows <code>x</code> to be arbitrarily large: data can be stored on disk and loaded as a memory-mapped array.</p>
<p>Instead of computing the entire FFT of the input array <code>x</code> (like <code>texshade.texshadeFFT</code>), this function can either</p>
<ol>
<li><em>generate</em> the smaller spatial-domain filter (the approximation to the true Laplacian) to be a square array with dimensions <code>nDiameter</code> per side and for shading detail factor <code>alpha</code> (for tips on picking <code>alpha</code>, see notes for <code>texshade.texshadeFFT</code> above), or</li>
<li><em>reuse</em> the <code>filter</code> you've already generated via <code>texshade.makeFilter</code> (see below).</li>
</ol>
<p><strong>Background on <code>nDiameter</code></strong> In the <em>exact</em> fractional-Laplacian operator implemented in <code>texshade.texshadeFFT</code> (above), each pixel in the output theoretically gets a contribution from <em>each</em> pixel of the input. <em>This function</em>, in contrast, limits contributions to a given output pixel to just the <code>nDiameter</code> by <code>nDiameter</code> sub-array that surrounds it in the original.</p>
<p><strong>Tips on picking <code>nDiameter</code></strong> There's a risk to making <code>nDiameter</code> too small or too large:</p>
<ul>
<li>if this is too small, the approximation will be inaccurate and you'll get ugly output. You want it big enough so each <code>nDiameter</code> by <code>nDiameter</code> pixel sub-array of terrain <code>x</code> has a rich set of physical features.</li>
<li>If this is too large, then you'll run out of RAM performing even the small FFTs needed by the overlap-save fast-convolution.</li>
</ul>
<p>The three remaining keyword arguments are for the overlap-save ("ols") algorithm that does the fast-convolution and are important to understand when you need to texture-shade huge datasets. They are fully documented in the <a href="./texshade/texshade.py">docstring</a>, so I won't duplicate that here.</p>
<p>This function will also pass any other keyword arguments <code>kwargs</code> to the <a href="https://github.com/fasiha/overlap_save-py"><code>ols</code> overlap-save</a> module. This lets you override the Scipy FFT with, for example, multi-threaded PyFFTW, etc.</p>
<h3 id="makefilter"><code>makeFilter</code></h3>
<p>API:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="kw">def</span> makeFilter(shape: List[<span class="bu">int</span>], alpha: <span class="bu">float</span>, dtype<span class="op">=</span><span class="bu">float</span>) <span class="op">-&gt;</span> np.ndarray</span></code></pre></div>
<p><strong>This function returns the filter</strong> (i.e., the approximation to the fractional-Laplacian operator) to use with <code>texshade.texshadeSpatial</code>). The output array has dimensions <code>shape</code> and type <code>dtype</code>. If <code>shape</code> has just one element, the output array will be square. If your terrain is a Numpy array of type <code>float32</code>, pass in <code>dtype=numpy.float32</code>, otherwise it defaults to 64-bit floats.</p>
<p>The shading factor <code>alpha</code> is the same as <code>texshade.texshadeFFT</code> above.</p>
<p>That should be all you need to start using this library! The rest of this document is a tutorial that describes all the other pieces you are likely to need to use this library effectively.</p>
<h2 id="tutorial">Tutorial</h2>
<p>Let's work through an the entire pipeline that texture shading is a part of:</p>
<ul>
<li>getting some raw digital elevation data as multiple files,</li>
<li>merging them into a single file,</li>
<li>applying the texture shading algorithm,</li>
<li>quantizing the results so each pixel is a byte (256 levels), and</li>
<li>emitting a georegistered texture-shaded terrain as PNG, ready for the web, QGIS, etc.</li>
</ul>
<h3 id="get-the-tutorial-code">Get the tutorial code</h3>
<p>There are a few useful scripts in the <a href="./tutorial">./tutorial</a> directory of this repo. Download them individually, or just clone this repo (install <a href="https://git-scm.com/">Git</a>, run <code>git clone https://github.com/fasiha/texshade-py.git; cd texshade-py/tutorial</code>).</p>
<h3 id="install-dependencies">Install dependencies</h3>
<p>This section is about installing GDAL, imagemagick, and two Python libraries, Pillow and <code>texshade</code> itself. Skip to the <a href="#download-data">data</a> step if you have all those installed.</p>
<p>Because setting up GDAL is often a tricky and laborious process, there are many tutorials online‚ÄîI'd like to share my approach because it's easy, reliable, cross-platform.</p>
<h4 id="install-miniconda">Install Miniconda</h4>
<p>Install <a href="https://docs.conda.io/en/latest/miniconda.html">miniconda</a>, a small command-line application that lets you create conda-based virtual environments, and download/install dependencies.</p>
<blockquote>
<p>Conda is awesome. I avoided it for years because it seemed corporate (Enthought), and because I thought I didn't need another Python environment manager beyond venv? , But conda-forge is a fully volunteer-run organization that packages all kinds of dependencies for all feasible operating systems and CPU architectures. So it's perfect for us to install the C++ and Python GDAL tools.</p>
</blockquote>
<h4 id="create-a-new-conda-env-and-install-dependencies">Create a new conda env and install dependencies</h4>
<div class="sourceCode" id="cb6"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1"></a><span class="co"># create environment</span></span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="ex">conda</span> create -n texshade-tutorial</span>
<span id="cb6-3"><a href="#cb6-3"></a><span class="co"># &quot;enter&quot; the environment</span></span>
<span id="cb6-4"><a href="#cb6-4"></a><span class="ex">conda</span> activate texshade-tutorial</span>
<span id="cb6-5"><a href="#cb6-5"></a><span class="co"># install binary dependencies</span></span>
<span id="cb6-6"><a href="#cb6-6"></a><span class="ex">conda</span> install -c conda-forge gdal Pillow imagemagick</span>
<span id="cb6-7"><a href="#cb6-7"></a><span class="co"># install this repo</span></span>
<span id="cb6-8"><a href="#cb6-8"></a><span class="ex">pip</span> install texshade</span></code></pre></div>
<p>We ask conda to install <code>gdal</code>, which is a Swiss Army chainsaw for geographic data process. <code>Pillow</code> and <code>imagemagick</code> are used by the tutorial to create and manipulate images. Pip is used to install <code>texshade</code> (this library) from PyPI since I haven't created a conda-forge recipe for it.</p>
<h3 id="download-data">Download data</h3>
<p>Download some data! I've downloaded three tiles from the SRTM DEM (from <a href="https://dwtkns.com/srtm30m/" class="uri">https://dwtkns.com/srtm30m/</a>) on the African coastline near 0¬∞ N and 0¬∞ W, because I've been loving John K Thornton's <em>Africa and Africans in the Making of the Atlantic World, 1400-1800</em>:</p>
<ul>
<li>N00E009.SRTMGL1.hgt.zip</li>
<li>N00E010.SRTMGL1.hgt.zip</li>
<li>N00E011.SRTMGL1.hgt.zip</li>
</ul>
<p>Unzip all three‚Äîa useful shell script for macOS/Linux/WSL:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1"></a><span class="kw">for</span> <span class="ex">i</span> in *.hgt.zip<span class="kw">;</span> <span class="kw">do</span> <span class="fu">unzip</span> <span class="va">$i</span><span class="kw">;</span> <span class="kw">done</span></span></code></pre></div>
<p>and then combine them into a single image, <code>merged.tif</code>:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1"></a><span class="ex">gdalwarp</span> -of GTiff N00E009.hgt N00E010.hgt N00E011.hgt merged.tif</span></code></pre></div>
<p>Running <code>gdalinfo merged.tif</code> produces the following output:</p>
<pre><code>Driver: GTiff/GeoTIFF
Files: merged.tif
Size is 10801, 3601
...
Origin = (8.999861111111111,1.000138888888889)
Pixel Size = (0.000277777777778,-0.000277777777778)
...
Corner Coordinates:
Upper Left  (   8.9998611,   1.0001389) (  8d59&#39;59.50&quot;E,  1d 0&#39; 0.50&quot;N)
Lower Left  (   8.9998611,  -0.0001389) (  8d59&#39;59.50&quot;E,  0d 0&#39; 0.50&quot;S)
Upper Right (  12.0001389,   1.0001389) ( 12d 0&#39; 0.50&quot;E,  1d 0&#39; 0.50&quot;N)
Lower Right (  12.0001389,  -0.0001389) ( 12d 0&#39; 0.50&quot;E,  0d 0&#39; 0.50&quot;S)
Center      (  10.5000000,   0.5000000) ( 10d30&#39; 0.00&quot;E,  0d30&#39; 0.00&quot;N)
Band 1 Block=10801x1 Type=Int16, ColorInterp=Gray
  NoData Value=-32768
  Unit Type: m</code></pre>
<p>This looks good: we have a 10801 by 3601 image whose center is close to the equator, as expected.</p>
<h3 id="convert-the-elevation-data-to-a-numpy-array">Convert the elevation data to a Numpy array</h3>
<p>To confine GDAL and geo-registered images to the edges of my workflow, I want to convert this elevation data to a simple Numpy array. <a href="./tutorial/convert.py"><code>convert.py</code></a> does that, so run it:</p>
<pre><code>python convert.py</code></pre>
<p>This creates a new file, <code>merged.tif.npy</code>.</p>
<h3 id="run-texshade">Run texshade!</h3>
<p>It's time to apply the texture-shading algorithm‚Äîwhat you've all come for! <a href="tutorial/demo.py"><code>demo.py</code></a> exercises the <code>texshade</code> library published by this repo. I've picked Œ± of 0.8, and it runs the memory-intensive <code>texshadeFFT</code> implementation.</p>
<pre><code>python demo.py</code></pre>
<p>This creates a new file, <code>merged.tif.npy.tex.npy</code> (note the "tex" in the filename).</p>
<h3 id="clamp-quantize-and-export">Clamp, quantize, and export</h3>
<p>I always clamp the texture-shaded array to between, say, 1-percentile and 99-percentile, to improve the base contrast. (Apps, like my <a href="https://fasiha.github.io/post/texshade/">Texture-Shaded Globe</a>, let you add even more contrast.)</p>
<p>Then I quantize the floating-point data to an 8-bit PNG, as well as a georegistererd TIF. The PNG is great for the web while the GeoTIFF is great for QGIS, etc.</p>
<p>This is all done in <a href="tutorial/postprocess.py"><code>postprocess.py</code></a>. Run it</p>
<pre><code>python postprocess.py</code></pre>
<p>to create</p>
<ul>
<li><code>scaled.tif</code>, the texture-shaded GeoTIFF,</li>
<li><code>scaled.png</code> , the texture-shaded 8-bit PNG, and</li>
<li><code>orig.png</code>, the original DEM data as an 8-bit PNG, for comparison.</li>
</ul>
<h3 id="rescale-for-the-web">Rescale for the web</h3>
<p>The PNG generated in the above image is way too big for the web. This next Bash command uses imagemagick's <code>convert</code> (installed by conda above) to resize the output images so I can include them in this repo.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb13-1"><a href="#cb13-1"></a><span class="kw">for</span> <span class="ex">i</span> in orig.png scaled.png<span class="kw">;</span> <span class="kw">do</span></span>
<span id="cb13-2"><a href="#cb13-2"></a>  <span class="ex">convert</span> -filter Mitchell -sampling-factor 1x1 -quality 90 -resize 2048 <span class="va">$i</span> <span class="va">$i</span>.small.png<span class="kw">;</span></span>
<span id="cb13-3"><a href="#cb13-3"></a><span class="kw">done</span></span></code></pre></div>
<h4 id="original-dem">Original DEM</h4>
<p><img src="tutorial/orig.png.small.png" alt="original downsampled" /></p>
<h4 id="tex-shaded-dem">Tex-shaded DEM</h4>
<p><img src="tutorial/scaled.png.small.png" alt="tex-shaded downsampled" /></p>
<h3 id="spatial-filtering-and-fast-convolution-for-low-memory-usage">Spatial filtering and fast-convolution for low-memory usage</h3>
<p>When we called <code>texshadeFFT</code> above, Python computed the two-dimensional FFT of the <em>entire</em> elevation array. This means that your computer had enough memory to store</p>
<ol>
<li>not just the entire elevation array but also</li>
<li>its (real-only) Fourier transform,</li>
<li>the fractional-Laplacian frequency-domain filter $$|\vec f|^Œ±$$.</li>
</ol>
<p>Even if we didn't store #3 above (e.g., if we used Numba to modify #2 in-place), since FFTW cannot do in-place Fourier transforms, we're still left with needing 3√ó the entire elevation array in free memory.</p>
<p>Imagine that we wanted to texture-shade our data on a tiny computer with a huge disk. We can load it as a memory-mapped file, so Numpy only reads the chunks of data it needs from disk to RAM, and run <code>texshadeSpatial</code>. This is demonstrated in <a href="./tutorial/memmap.py">memmap.py</a>. Run it:</p>
<pre><code>python memmap.py</code></pre>
<p>to produce a file, <code>mmap.png</code>.</p>
<p>I used a 500 by 500 spatial filter: each pixle in the output image received contributions from the 500 by 500 pixel grid around it in the input, and <em>no</em> contribution from pixels outside that neighborhood.</p>
<p>The keyword argument <code>size=[1500, 2000]</code> was provided to <code>texshadeSpatial</code>. As explained in the docstring, this means that the overlap-save algorithm will take two-dimensional FFTs of roughly <code>size + nDiameter - 1 = [1999, 2499]</code>. (We zeropad the input to FFT sizes with small prime factors, so each FFT will actually be 2000 by 2500.) This is much smaller than a full 2D FFT of the entire 3601 by 10801 input array.</p>
<p>Note that the overlap-save algorithm, while overall efficient, is going to be slower than the single-large-FFT in <code>texshadeFFT</code>. Furthermore, memory-mapping and disk-I/O is also going to be much slower than RAM. As is hopefully clear, you only want to use <code>texshadeSpatial</code> if you don't have the RAM for <code>texshadeFFT</code>.</p>
<p>Let's generate a few crops (using <code>convert</code> from <code>imagemagick</code>, installed by conda above):</p>
<pre><code>for i in mmap.png scaled.png; do 
  convert -crop 500x500+1090+1341 $i crop1-$i;
  convert -crop 500x500+3300+202 $i crop2-$i;
  convert -crop 500x500+7054+1968 $i crop3-$i;
done</code></pre>
<p>Inspecting these, the only differences I see are due to slight differences in the contrast.</p>
<h4 id="crop-1">Crop 1</h4>
<p><img src="tutorial/crop1-scaled.png" alt="Exact FFT, crop 1" /></p>
<p><img src="tutorial/crop1-mmap.png" alt="Approximate spatial filter, crop 1" /></p>
<h4 id="crop-2">Crop 2</h4>
<p><img src="tutorial/crop2-scaled.png" alt="Exact FFT, crop 2" /></p>
<p><img src="tutorial/crop2-mmap.png" alt="Approximate spatial filter, crop 2" /></p>
<h4 id="crop-3">Crop 3</h4>
<p><img src="tutorial/crop3-scaled.png" alt="Exact FFT, crop 3" /></p>
<p><img src="tutorial/crop3-mmap.png" alt="Approximate spatial filter, crop 3" /></p>
<p>We can finally run the texture shading algorithm on enormous datasets without needing gargantuan amounts of memory!</p>
<h2 id="developing-in-this-repository">Developing in this repository</h2>
<p>To build the HTML, I use Pandoc and coordinate it with the <a href="./Makefile"><code>Makefile</code></a>, which can be invoked by running <code>$ make</code>.</p>
